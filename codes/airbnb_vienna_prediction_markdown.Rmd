---
title: "Vienna AirBnb Price Prediction"
author: "John Joyce"
date: "2/2/2021"
output: html_document
code_download: yes
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
options(digits = 5)

################################################################################
###################                   VIENNA                ####################
###################               MID-SIZE AIRBNB           ####################
###################              PRICE PREDICTION           ####################
################################################################################



# This script uses a function written by Gabor Bekes and Gabor Kezdi for the code which accompanies their book:
# Data Analysis for Business, Economics, and Policy
# The function is called price_diff_by_variables
# Check out: gabors-data-analysis.com and github.com/gabors-data-analysis/
# Thank you Gabor for bring energy to Zoom lectures!



################################################################################
###################                                         ####################
###################              PREPARE DATA               ####################
###################                                         ####################
################################################################################



# load libraries
library(tidyverse)
library(caret)
library(ranger)


# SET WORKING DIRECTORY  TO YOUR PREFERRED PATH
dir <- getwd()

# github URL for loading data from my repo
github_url <- 'https://raw.githubusercontent.com/joyce-john/DA3_A1/main/data/clean/'

# location folders - assuming you are downloading from GitHub and not using any special folder structure when running
data_in <- github_url
data_out <- dir

# # location folders - for local work
# data_in  <- paste0(dir,"/data/clean/")
# data_out <- paste0(dir,"/data/clean/")


# read the data into a dataframe
df <- read_csv(paste0(data_in, 'airbnb_vienna_midsize_clean.csv'))


# take a look at the situation with NAs
# all NAs were dealt with in the cleaning script, except for bedrooms
na_info <- df %>% select(-c('id', 'name')) %>% summarise_all(funs(sum(is.na(.))))
na_info[,colSums(na_info>0)>0]


# change some features to factors
df <- 
  df %>% 
  mutate_if(is.character, factor) %>% 
  mutate(bedrooms = factor(bedrooms, levels = c(1, 2, 3, 4, 5, NA), exclude = NULL)) # setting to factor to allow use of NAs in ML



################################################################################
###################              FUNCTIONAL FORM            ####################
###################              CONSIDERATIONS:            ####################
###################           UNCOMMENT TO SEE CHARTS       ####################
################################################################################



# ##############################################################################
# 
# # THIS SECTION COMMENTED OUT SO THE SCRIPT CAN RUN FASTER
# # SUMMARY: no evidence to suggest that we should alter the functional form of the variables
# 
# 
# # Look at some scatterplots to answer key questions about functional form...
# # Should we take log of our y variables, price, because its distribution is skewed?
# # Should we transform any of our other x variables (log, quadratic, cubic, whatever)...
# # ...to better accommodate a certain pattern of association with price or log price?
# 
# # number of reviews and price
# df %>% ggplot(aes(x = number_of_reviews, y = price)) + geom_point(alpha = 0.5) + geom_smooth(method = 'loess')
# 
# # number of reviews and **log price**
# df %>% ggplot(aes(x = number_of_reviews, y = ln_price)) + geom_point(alpha = 0.5) + geom_smooth(method = 'loess')
# 
# # **log number of reviews** and price
# df %>% ggplot(aes(x = ln_num_reviews, y = price)) + geom_point(alpha = 0.5) + geom_smooth(method = 'loess')
# 
# # **log number of reviews** and **log price**
# df %>% ggplot(aes(x = ln_num_reviews, y = ln_price)) + geom_point(alpha = 0.5) + geom_smooth(method = 'loess')
# 
# # accommodates and price
# df %>% ggplot(aes(x = accommodates, y = price)) + geom_point(alpha = 0.5) + geom_smooth(method = 'loess')
# 
# # accommodates and **log price**
# df %>% ggplot(aes(x = accommodates, y = ln_price)) + geom_point(alpha = 0.5) + geom_smooth(method = 'loess')
# 
# # bathrooms and price
# df %>% ggplot(aes(x = bathrooms, y = price)) + geom_point(alpha = 0.5) + geom_smooth(method = 'loess')
# 
# # bathrooms and **log price**
# df %>% ggplot(aes(x = bathrooms, y = ln_price)) + geom_point(alpha = 0.5) + geom_smooth(method = 'loess')
# 
# # bedrooms and price
# df %>% ggplot(aes(x = as.numeric(bedrooms), y = price)) + geom_point(alpha = 0.5) + geom_smooth(method = 'loess')
# 
# # bedrooms and **log price**
# df %>% ggplot(aes(x = as.numeric(bedrooms), y = ln_price)) + geom_point(alpha = 0.5) + geom_smooth(method = 'loess')
# 
# # After looking at these graphs, my answer is "no". 
# # There are some quadratic-looking patterns here, like price/ln_price & bathrooms. But it's convex! 
# # That doesn't seem to make sense, and I think its due to the lower number of "4 bathrooms" obs.
# 
# # Okay, just one more consideration: should I at least make a binary variable "no_reviews"
# # to capture any differences in price between brand new apartments and well-established ones?
# 
# # create a no_reviews binary variable, and compare mean prices
# df %>% 
#   mutate(no_reviews = ifelse(number_of_reviews == 0, TRUE, FALSE)) %>% 
#   ggplot(aes(x = no_reviews, y = price)) + 
#   geom_jitter(color="black", size=0.2, alpha=0.3) +
#   geom_boxplot()
# 
# # It's not what I expected to find...
# # the boxplot doesn't suggest that zero reviews has a special association with price.
# 
# ##############################################################################



################################################################################
###################                                         ####################
###################          CONSIDER INTERACTIONS          ####################
###################                                         ####################
################################################################################



# SUMMARY: parking variables definitely interact with neighbourhood in a meaningful way
# CONT... I'm not 100% confident in the others; I will include them and let LASSO decide

# define a function to create plots which help identify potential interactions
# this function was written by Gabor Bekes and  Gabor Kezdi and modified slightly
price_diff_by_variables <- function(df, factor_var, dummy_var){
  # Looking for interactions.
  # It is a function it takes 3 arguments: 1) Your dataframe,
  # 2) the factor variable (like room_type)
  # 3)the dummy variable you are interested in (like TV)
  
  # Process your data frame and make a new dataframe which contains the stats
  
  factor_var <- as.name(factor_var)
  dummy_var <- as.name(dummy_var)
  
  stats <- df %>%
    group_by(!!factor_var, !!dummy_var) %>%
    dplyr::summarize(Mean = mean(price, na.rm=TRUE),
                     se = sd(price)/sqrt(n()))
  
  stats[,2] <- lapply(stats[,2], factor)
  
  ggplot(stats, aes_string(colnames(stats)[1], colnames(stats)[3], fill = colnames(stats)[2]))+
    geom_bar(stat='identity', position = position_dodge(width=0.9))+
    geom_errorbar(aes(ymin=Mean-(1.96*se),ymax=Mean+(1.96*se)),
                  position=position_dodge(width = 0.9), width = 0.25)+
    ylab('Mean Price')+
    theme_bw()+
    theme(panel.grid.major=element_blank(),
          panel.grid.minor=element_blank(),
          panel.border=element_blank(),
          axis.line=element_line(),
          axis.text.x = element_text(angle = 90, vjust = 0.5, hjust=1)) + #John's modification
    labs(x = 'Neighbourhood') + # John's modification
    scale_fill_grey()
}

# now, let's consider whether there any interactions with neighbourhood and other variables
# take a look at interactions with some of the basic listing information variables
# I can't examine all 100+ amenities, but domain knowledge suggests we should consider PARKING

#Look up parking (free/paid) interactions with neighbourhood
p1 <- price_diff_by_variables(df, "neighbourhood_cleansed", "Free_parking")
p2 <- price_diff_by_variables(df, "neighbourhood_cleansed", "Paid_parking")

# room_type?
p3 <- price_diff_by_variables(df, "neighbourhood_cleansed", "room_type" )

# accommodates?
p4 <- price_diff_by_variables(df, "neighbourhood_cleansed", "accommodates")

# instant bookable?
p5 <- price_diff_by_variables(df, "neighbourhood_cleansed", "instant_bookable")

# these plots are crowded, so I examine them one-by-one
p1
p2
p3
p4
p5

# Without examining *all* the variable interactions...
# I can see that some basic variables interact with neighbourhood but others don't (shocker)
# There is no way I could test all the amenities by hand, and there is no point in thinking too hard
# about which interactions to include and which ones to throw away, when LASSO can make this decision for me
# I will throw a bunch of neighbourhood* interactions into the mix, because *some* of them will be useful
# and the others will get zero'd out by LASSO



################################################################################
###################                  FORMULAS                   ################
###################              MODEL PARAMETERS               ################
###################              TRAIN & TEST SETS              ################
################################################################################



########################
#####   FORMULAS    ####
########################


# put variables in groups to make model-building easier

# basic characteristics of the listing
basic_vars <- c('neighbourhood_cleansed', 
                'room_type', 
                'accommodates', 
                'bathrooms', 
                'bedrooms', 
                'beds', 
                'instant_bookable', 
                'shared_bathroom',
                'number_of_reviews')

# interactions with neighbourhood
# I can't explore all possible interactions, so I'm only using basic property stats + parking
neighbourhood_interactions <- c('room_type*neighbourhood_cleansed', 
                                'accommodates*neighbourhood_cleansed', 
                                'bathrooms*neighbourhood_cleansed', 
                                'bedrooms*neighbourhood_cleansed', 
                                'beds*neighbourhood_cleansed', 
                                'instant_bookable*neighbourhood_cleansed', 
                                'shared_bathroom*neighbourhood_cleansed',
                                'number_of_reviews*neighbourhood_cleansed',
                                'Free_parking*neighbourhood_cleansed',
                                'Paid_parking*neighbourhood_cleansed')

# dummy variables for the amenities (fridges, parking, etc.) in columns 15 - 123
amenities <- colnames(df[15:123])


# simplify model formulas with these groups
predictors_1 <- basic_vars
predictors_2 <- c(basic_vars, amenities)
predictors_3 <- c(basic_vars, amenities, neighbourhood_interactions)


# set formulas

# formula 1: basic variables
formula_1 <- 
  as.formula(
    paste0('price ~ ', 
           paste0(predictors_1,
                  collapse = ' + ')))

# formula 2: basic variables + amenities
formula_2 <- as.formula(
  paste0('price ~ ', 
         paste0(predictors_2,
                collapse = ' + ')))

# formula 3: basic variables + amenities + neighbourhood interactions
formula_3 <- as.formula(
  paste0('price ~ ', 
         paste0(predictors_3,
                collapse = ' + ')))


################################
#####   MODEL PARAMETERS    ####
################################


# set trainControl to do 5 folds of cross validation
train_control <- trainControl(method = "cv", number = 5)

# tuneGrid for LASSO
# try lambdas all the way up 1, we need to allow LASSO to basically zero things out if appropriate
tunegrid_lasso <-  expand.grid("alpha" = 1, "lambda" = seq(0.05, 1, by = 0.01))

# set tunegrid for random forest: simple model
# for the simple model, we have 9 features. we may consider sqrt(9) = 3 as a potential value for m
# let's at least give it two options: 3 and 4. if we go any higher, the trees may get too correlated
tune_grid_rf_simple <- expand.grid(
  .mtry = c(3, 4),
  .splitrule = "variance",
  .min.node.size = c(5, 10)
)

# set tunegrid for random forest: full model
# for the full model, we have 121 features, we may consider sqrt(121) = 11 as a possible value for m
# but let's have it try a few values
tune_grid_rf_full <- expand.grid(
  .mtry = c(9, 11, 13),
  .splitrule = "variance",
  .min.node.size = c(5, 10)
)

# setting train control for RF - the same as previous train control but with verboseIter = TRUE
# so I can see the progress
train_control_rf <- trainControl(method = "cv", number = 5, verboseIter = TRUE)


################################
#####    TRAIN/TEST SETS    ####
################################


# set the seed for reproducibility
set.seed(1991)

# create an index for splitting the data. let's do a classic 80/20 split
# use as.integer() to coerce the matrix into a vector
index <- as.integer(
  createDataPartition(df$price, times = 1, p = 0.8, list = FALSE)
)

# create sets with index
train_set <- df[index,]
test_set <- df[-index,]



################################################################################
###################                                             ################
###################               CREATE MODELS                 ################
###################                                             ################
################################################################################



# OLS

# linear model 1: basic predictors
lm_1 <- caret::train(formula_1,
                    data = train_set,
                    method = 'lm',
                    preProcess = c('center', 'scale'),
                    trControl = train_control)

# linear model 2: basic predictors + amenities
lm_2 <- caret::train(formula_2,
                    data = train_set,
                    method = 'lm',
                    preProcess = c('center', 'scale'),
                    trControl = train_control)

# linear model 3: basic predictors + amenities + neighbourhood interactions
lm_3 <- caret::train(formula_3,
                    data = train_set,
                    method = 'lm',
                    preProcess = c('center', 'scale'),
                    trControl = train_control)

# LASSO

# give LASSO all potential x variables and let the smartypants algo decide what's important
lasso_1 <- caret::train(formula_3,
                       data = train_set,
                       method = 'glmnet',
                       preProcess = c('center', 'scale'),
                       tuneGrid = tunegrid_lasso,
                       trControl = train_control
)

# Random Forest

# first train a simple model only using basic property stats
rf_1 <- train(
  formula_1,
  data = train_set,
  method = "ranger",
  trControl = train_control_rf,
  tuneGrid = tune_grid_rf_simple,
  importance = "impurity"
)

# second train a complex model using all features
rf_2 <- train(
  formula_3,
  data = train_set,
  method = "ranger",
  trControl = train_control_rf,
  tuneGrid = tune_grid_rf_full,
  importance = "impurity"
)



################################################################################
#################                                               ################
#################               EVALUATE MODELS                 ################
#################                                               ################
################################################################################



######################################
#####    CROSS-VALIDATED RMSE    #####
######################################


# put all models into a list
final_models <-
  list("OLS_1" = lm_1,
       "OLS_2" = lm_2,
       "OLS_3" = lm_3,
       "LASSO" = lasso_1,
       "Random_forest_basic" = rf_1,
       "Random forest_full" = rf_2)

# summarize model stats
results <- resamples(final_models) %>% summary()

# examine cross-validated RMSE
results$statistics$RMSE

# LASSO and Random Forest (full model) have the lowest mean cross-validated RMSE


######################################
#####       TEST-SET RMSE        #####
######################################


# make predictions  on the **test set**
predictions_lm_1 <- predict(lm_1, test_set)
predictions_lm_2 <- predict(lm_2, test_set)
predictions_lm_3 <- predict(lm_3, test_set)
predictions_lasso_1 <- predict(lasso_1, test_set)
predictions_rf1 <- predict(rf_1, test_set)
predictions_rf2 <- predict(rf_2, test_set)

# calculate RMSE for test set predictions
test_set_rmse <- data.frame('OLS_1' = RMSE(predictions_lm_1, test_set$price), 
                            'OLS_2' = RMSE(predictions_lm_2, test_set$price),
                            'OLS_3' = RMSE(predictions_lm_3, test_set$price),
                            'LASSO' = RMSE(predictions_lasso_1, test_set$price),
                            'Random_forest_basic' = RMSE(predictions_rf1, test_set$price),
                            'Random_forest_full' = RMSE(predictions_rf2, test_set$price))

# LASSO and Random Forest (full model) still have the best RMSE
# and it's similar to the training set RMSE, which is a good sign that we didn't overfit
print(test_set_rmse)

# The winner? Random forest (full model) for having the lowest RMSE.


##################################################
#####        DIAGNOSTICS: Y-hat VS Y         #####
##################################################

# LASSO - scatterplot of predicted VS actual values
lasso_pred_vs_actual <- 
  test_set %>% 
  mutate(predictions = predictions_lasso_1) %>% 
  ggplot(aes(x = predictions, y = price)) +
  geom_point(color = 'blue', size = 1, shape = 16, alpha = 0.7, show.legend=FALSE, na.rm=TRUE) +
  geom_segment(aes(x = 0, y = 0, xend = 350, yend =350), size=0.5, color = 'red', linetype=2) +
  coord_cartesian(xlim = c(0, 350), ylim = c(0, 350)) +
  scale_x_continuous(expand = c(0.01,0.01),limits=c(0, 350), breaks=seq(0, 350, by=50)) +
  scale_y_continuous(expand = c(0.01,0.01),limits=c(0, 350), breaks=seq(0, 350, by=50)) +
  labs(title = 'LASSO', y = "Price (EUR)", x = "Predicted price  (EUR)") +
  theme(plot.title = element_text(hjust = 0.5))

# RANDOM FOREST (full) - scatterplot of predicted VS actual values
rf_2_pred_vs_actual <-
  test_set %>% 
  mutate(predictions = predictions_lasso_1) %>% 
  ggplot(aes(x = predictions, y = price)) +
  geom_point(color = 'blue', size = 1, shape = 16, alpha = 0.7, show.legend=FALSE, na.rm=TRUE) +
  geom_segment(aes(x = 0, y = 0, xend = 350, yend =350), size=0.5, color = 'red', linetype=2) +
  coord_cartesian(xlim = c(0, 350), ylim = c(0, 350)) +
  scale_x_continuous(expand = c(0.01,0.01),limits=c(0, 350), breaks=seq(0, 350, by=50)) +
  scale_y_continuous(expand = c(0.01,0.01),limits=c(0, 350), breaks=seq(0, 350, by=50)) +
  labs(title = 'Random Forest (full model)', y = "Price (EUR)", x = "Predicted price  (EUR)") +
  theme(plot.title = element_text(hjust = 0.5))

# show LASSO scatter
lasso_pred_vs_actual

# show RF scatter
rf_2_pred_vs_actual


##########################################################
#####        DIAGNOSTICS: LASSO COEFFICIENTS         #####
##########################################################


# get the coefficients from the LASSO model
lasso_coeffs <- coef(lasso_1$finalModel, lasso_1$bestTune$lambda) %>%
  as.matrix() %>%
  as.data.frame() %>%
  rownames_to_column(var = "variable") %>%
  rename(coefficient = `1`)  # the column has a name "1", to be renamed

# NOT RUN: print out all the coefficients if you want to gaze at all the 0s
#print(lasso_coeffs)

# count the coefficients which are non-zero
lasso_coeffs_nz <- lasso_coeffs %>% filter(coefficient!=0)

# there are 184 non-zero coefficients
print(nrow(lasso_coeffs_nz))

# get the top 10 LASSO coefficients
lasso_top10_coef <-
  lasso_coeffs %>% 
  arrange(desc(coefficient)) %>% 
  head(., n = 10)

# view the top 10 LASSO coefficients
lasso_top10_coef

# some of these make perfect sense such as accomoodates, TV, Air_conditioning
# but it's surprising to see that Innere Stadt bathrooms could be so significant


#########################################################################
#####        DIAGNOSTICS: RANDOM FOREST VARIABLE IMPORTANCE         #####
#########################################################################


# calculate random forest variable importance as a percentage
rf_2_var_imp <- importance(rf_2$finalModel)/1000
rf_2_var_imp_df <-
  data.frame(varname = names(rf_2_var_imp), imp = rf_2_var_imp) %>%
  mutate(varname = gsub("room_typePrivate room", "room type: private room", varname) ) %>%
  mutate(varname = gsub("bedrooms2", "bedrooms: 2", varname) ) %>%
  mutate(varname = gsub("neighbourhood_cleansedInnere Stadt:bathrooms", "Innere Stadt * bathrooms", varname) ) %>%
  mutate(varname = gsub("neighbourhood_cleansedInnere Stadt:accommodates", "Innere Stadt * accommodates", varname) ) %>%
  arrange(desc(imp)) %>%
  mutate(imp_percentage = imp/sum(imp))

# make plot of top 10 varimp vars
rf_2_var_imp_plot <- ggplot(rf_2_var_imp_df[1:10,], aes(x=reorder(varname, imp), y=imp_percentage)) +
  geom_point(color= 'blue', size=1) +
  geom_segment(aes(x=varname,xend=varname,y=0,yend=imp_percentage), color= 'blue', size=0.75) +
  ylab("Importance (Percent)") +
  xlab("Variable Name") +
  labs(title = "Top 10 Variables (VarImp)") +
  coord_flip() +
  scale_y_continuous(labels = scales::percent_format(accuracy = 1)) +
  theme(axis.text.x = element_text(size=8), axis.text.y = element_text(size=8),
        axis.title.x = element_text(size=8), axis.title.y = element_text(size=8),
        plot.title = element_text(hjust = 0.5))

# show plot - it's tempting to group all the bathroom vars, but I don't want to hide the message
# number of bathrooms matters, so does whether it's shared or not, bathrooms in the Innere Stadt are special
rf_2_var_imp_plot


#########################################################################
#####         DIAGNOSTICS: RANDOM FOREST PARTIAL DEPENDENCE          ####
#########################################################################


# partial dependence plot for number of people it accommodates
pdp_n_acc <- pdp::partial(rf_2, pred.var = "accommodates", pred.grid = distinct_(test_set, "accommodates"), train = train_set)
pdp_n_acc_plot <- pdp_n_acc %>%
  autoplot( ) +
  geom_point(color= 'blue', size=2) +
  geom_line(color= 'red', size=1) +
  ylab("Predicted price") +
  xlab("Accommodates (persons)") +
  labs(title = "Partial Dependence Plot - Accommodates") +
  scale_x_continuous(limit=c(2,6), breaks=seq(2,6,1)) +
  theme(plot.title = element_text(hjust = 0.5))

# show the plot
pdp_n_acc_plot


# partial dependence plot for number of bathrooms
pdp_bathrooms <- pdp::partial(rf_2, pred.var = "bathrooms", pred.grid = distinct_(test_set, "bathrooms"), train = train_set)
pdp_bathrooms_plot <- pdp_bathrooms %>%
  autoplot( ) +
  geom_point(color= 'blue', size = 2) +
  geom_line(color= 'red', size = 1) +
  ylab("Predicted price") +
  xlab("Number of Bathrooms") +
  scale_x_continuous(limit=c(0.5,4), breaks=seq(0.5,4,0.5)) +
  labs(title = "Partial Dependence Plot - Bathrooms")+
  theme(plot.title = element_text(hjust = 0.5)) 

# show the plot
# this is certainly an interesting shape
# finding patterns like this is the strength of machine learning
pdp_bathrooms_plot

```
# Introduction  
My task is to help a company offering small and mid-size apartments find the right price for their new AirBnb units in Vienna. I do this by building a model for predicting Airbnb prices with publicly available data from [InsideAirbnb](http://insideairbnb.com/get-the-data.html). We can run the model on the client's apartments to figure out what an appropriate price is for each unit.  
  
The Vienna AirBnb data has approximately 10.000 observations after cleaning and preparation. The **mean price** is about €68 and the **average number of reviews** is 33. However, 17% of the apartments in the data have 0 reviews, just like the client's properties.

# Cleaning and Filtering  
In the cleaning and filtering stage, I need to accomplish two goals:  
* adjusting the sample to match this specific task  
* getting the data ready for machine learning  

### Adjusting the Sample
The data contains 61 different property types. Our client only has one property type: **apartment**. However, if we take a closer look, we see that 87% of the data are **Entire apartment** or **private room in apartment**, which are appropriate for our sample. I keep any property which is an apartment of some kind, and I filter out anything that is not.  
  
The client's apartments accommodate **two to six guests**. Therefore, I filter out any apartments which are not in this range. 

I also filter out apartments which have **extreme values for price**. I do this for two reasons. First, I do not believe that the client has luxury apartments, so having luxury apartments in the sample for the pricing model is not helpful. Second, these values (whether they are real values for luxury apartments or mistakes) are going to hurt the accuracy of the models. I set the cutoff at €600 per night.   
  
### Getting the data ready for machine learning
The data needs to be in a clean format before it can be passed to a model. The key tasks are:  
* dealing with NAs  
* feature engineering with clean variable names/types  
* dropping unneeded variables  

Some important columns have NAs. When these occur in the **price** column, they must be dropped. When these occur in the columns which are used as predictive features, they can be dropped or imputed if the number is relatively small. However, there is one difficult case: the **bedrooms** column has more than 1000 NAs in the original data. This is about 10% of the data after filtering. I take a conservative approach here: I won't impute or drop these values, because there could be a systemic reason that the information is missing (ex. hosts are hiding the information because it's unfavorable). I leave these observations in the data, and I set **NA** as a **factor level** when doing prediction. 
  
I do a small amount of feature engineering. Every row in the **amenities** column contains a list which no machine learning tool will be able to interpret. I make dummy variables for every unique value in **amenities**, which describes all the small features that are included with an Airbnb (appliances, sound systems, patios, etc.) To get the most out of this information, I consolidate amenities which are extremely similar, such as refrigerators of different brands and different varieties of stoves. Many variables are renamed to replace spaces with underscores.  
  
I drop unneeded variables to make the data easier to work with. Using domain knowledge or common sense, I drop anything I am confident will not help to predict **price**. For the client's case, I also drop **review scores** columns. This is really unfortunate, because these reviews would be useful predictive features. However, the client's apartments are **new to the market** and do not have any reviews. It would not be reasonable to impute values for the client to run the prediction model: we have no idea *how accurate customers will find the listing* or *how clean they will find it* or *how much they will like it overall*. I decide to keep the **number_of_reviews** variable because we do have this information (it's zero).  
  
# Functional Form and Interactions
I do not make any functional form adjustments. I examine scatterplots of numeric *x* variables and the *y* variable, **price**, with loess smoothing, and I don't find any evidence to suggest that a transformation (such a log, square, or cubic) would better accommodate the pattern of association between the variables. There are two numeric variables with skewed distributions, **number_of_reviews** and **price**, but log transformations of these variables don't seem to help fit any meaningful pattern with other variables.  
  
I decide to interact some variables with **neighbourhood**. The logic here is that property features be extremely important in some areas and less important in others. One crystal clear example is **free parking**. In some areas of the city, parking spots are precious. The graph below mean Airbnb prices, with and without **free parking**, by neighbourhood:  
  
```{r, echo = FALSE, warning = FALSE, message = FALSE}
p1
```
  
I can not test the interactions of every variable with **neighbourhood**, so I choose **free parking** and **paid parking**, as well as the basic variables which describe the listing such as the number of people it **accommodates** and whether it is **instant bookable** or not.  

# Building the Potential Models
I split the data into training and test sets. Then I train three types of models:  
1. OLS  
2. LASSO  
3. Random Forest  
  
For the OLS models, I consider three formulas with increasing levels of complexity:  
1. basic variables only  
2. basic variables + all amenities  
3. basic variables + all amenities + interactions with the **neighbourhood** variables  
  
For the LASSO model, I only use the most complex formula. LASSO will use a regularization parameter to zero out the least useful coefficients. The optimal value of this parameter is selected through 5-fold cross-validation.
  
For the Random Forest model, I make a basic version and a full version with all variables. Random Forest will select the most useful feature from a random selection of *m* variables at each tree split, so it can also handle a large variety of features. Of course, *m* is also optimized in cross-validation. However, I am curious if huge selection of variables actually helps. That is why I make two versions for the sake of comparison.    
  
# Model Evaluation
I start by taking a look at the cross-fold RMSE.  
  
```{r, echo = FALSE, warning = FALSE, message = FALSE}
library(kableExtra)
results$statistics$RMSE %>% kable() %>% kableExtra::kable_styling()
```  
  
LASSO and Random Forest (full model) have the lowest mean RMSE in cross-validation. The models are reasonably stable across the folds - we can see this by comparing the minimum and maximum RMSE values.  
  
The real question is: which model performs the best on the holdout data? I make sample predictions and calculate RMSE for each model on the test set.  
  
```{r, echo = FALSE, warning=FALSE, message=FALSE}
test_set_rmse %>% kable() %>% kable_styling()
```  
  
Random Forest (full model) is the clear winner. In fact, the RMSE is even lower in the holdout set than the average value for the training data. The client wants accurate pricing, and this one delivers the lowest degree of error, **therefore this is my chosen model**. The bottom line is that smaller errors mean more accurate pricing and more profit for the client.  
  
However, LASSO is a close second. If the client were not satisfied with a black-box model, I would offer the LASSO model as an alternative. It offers the advantage of interpretable coefficients and can be fit to new data in a fraction of the time. It has the second-lowest RMSE.  
  
# Model Diagnostics

### Predicted VS Actual Values  
The top two models make extremely similar predictions. The scatterplots below show the relationship between the **actual price in the holdout data** and the **price predicted by the models**. The plots do not help us distinguish the models, but they do offer some other insight: prices are usually less than 150 Euros, and the models tend to underestimate prices higher than that.  

```{r, echo = FALSE, warning=FALSE, message=FALSE}
rf_2_pred_vs_actual  
lasso_pred_vs_actual
```  
   
### Top 10 LASSO Coefficients  
The LASSO model has 184 non-zero coefficients, but many of them are very small. The top ten coefficients give us an idea about which variables are the most useful in explaining variation in price.
```{r, echo = FALSE, warning = FALSE, message = FALSE}

# remove intercept from list
lasso_top10_coef_clean <-
    lasso_coeffs %>% 
    arrange(desc(coefficient)) %>% 
    head(11) %>% 
    tail(10)

# rename ugly vars
lasso_top10_coef_clean[1,1] <- 'Innere Stadt * bathrooms'
lasso_top10_coef_clean[7,1] <- 'Innere Stadt * bedrooms3'

# show coefficients
lasso_top10_coef_clean %>% kable(row.names = F) %>% kable_styling()

```  
  
### Top 10 of Variable Importance for Random Forest  
*Variable importance* describes a variable's contribution to reducing overall RMSE in the Random Forest model. I examine the *percentage of total RMSE reduction* attributed to each variable, as it's a bit easier to interpret. The top 10 variables - by this metric of *variable importance* - are shown in the chart below:  
```{r, echo = FALSE, warning = FALSE, message = FALSE}
rf_2_var_imp_plot
```  
  
Although LASSO coefficients and variable importance values are not directly comparable, it is interesting to see how the top 10 lists from both metrics share many similarities.    
```{r, echo = FALSE, warning = FALSE, message = FALSE}
# make a dataframe for comparing most important variables
top10_compare <-
  data.frame('LASSO' = lasso_top10_coef_clean$variable, 
             'Random Forest' = rf_2_var_imp_df$varname[1:10])

top10_compare %>% kable() %>% kable_styling()
```  
  
### Partial Dependence Plots for Random Forest  
  
Though the Random Forest model does not have coefficients, a partial dependence plot can provide  insight about the relationship between an *x* variable and the target *y*. The plot below shows the average value for **price** for different values of **accommodates**, conditional on all other *x* variables.  

```{r, echo = FALSE, warning = FALSE, message = FALSE}
pdp_n_acc_plot
```  
  
Of course, the real strength of machine learning is its ability to uncover atypical patterns. The partial dependence plot for the **bathrooms** variable has an unusual shape.  
  
```{r, echo = FALSE, warning = FALSE, message = FALSE}
pdp_bathrooms_plot
```  
  
If the client were skeptical of a black-box model like Random Forest and wanted some assurance that the model operated in a reasonable way, I would share the partial dependence plots for the most important variables. In the case of a bad prediction, these plots may offer some insight into which variables threw the model off-target.  

# Conclusion  
The model I built is decent, but not perfect. The best model achieved an **RMSE of 36.349**, which is somewhat lower than the RMSE achieved in the London case study, but for a completely different city, range of values, and business case. My model has a narrower (easier) task: predicting prices only for typical apartments. The models in the London case study faced a broader range of values in the target  **price** variable and in other *x* variables.  

It may have been possible to improve my model's performance further by including information from the various **review scores** variables, but I am highly skeptical of the idea of imputing these values for the client's apartments. It would have made the model look better on paper, but may have resulted in worse predictions for the client if we incorrectly guess what the client's **review scores** should be.  
  
As for the client's use of the model: I would propose using the model as a starting point for setting prices. It may be a valuable complement to the client's domain expertise, but it is only a decent performer *on average*, and it would be wise to consider individual price adjustments if the prediction appears unreasonable.
  
